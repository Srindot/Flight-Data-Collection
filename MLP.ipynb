{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Preceptors\n",
    "\n",
    "#### Training the MLP model on Ptera Software Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Modules Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import joblib  # For saving the scaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Data to the pandas Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0.1    1  -30         0  13.92139289  1.855566073  0.294452854\n",
      "0      0.1  1.0  -30  0.500000    -9.041235    -4.439285    22.045514\n",
      "1      0.1  1.0  -25  0.000000    13.925155     1.909448     1.711641\n",
      "2      0.1  1.0  -25  0.500000   -11.013954    -4.369710    20.869233\n",
      "3      0.1  1.0  -20  0.000000    13.789185     1.963994     3.143793\n",
      "4      0.1  1.0  -20  0.500000   -12.863685    -4.297365    19.520126\n",
      "...    ...  ...  ...       ...          ...          ...          ...\n",
      "92151  1.2  6.0   30  0.970588   -14.949841     0.056181    -3.850756\n",
      "92152  1.2  6.0   30  0.976471   -14.939992     0.041060    -3.836797\n",
      "92153  1.2  6.0   30  0.982353   -14.929552     0.025673    -3.821214\n",
      "92154  1.2  6.0   30  0.988235   -14.918350     0.010004    -3.804004\n",
      "92155  1.2  6.0   30  0.994118   -14.906249    -0.005956    -3.785174\n",
      "\n",
      "[92156 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Reading the Data\n",
    "data = pd.read_csv(\"Mark3Data.csv\",header = 1)\n",
    "\n",
    "# Displaying the structure of the Data \n",
    "print(pd.DataFrame(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into Input Parameters and Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Input Parameters : \n",
      "       Flapping Frequency  Airspeed  Angle Of Attack  Normalised Time\n",
      "0                     0.1       1.0              -30         0.500000\n",
      "1                     0.1       1.0              -25         0.000000\n",
      "2                     0.1       1.0              -25         0.500000\n",
      "3                     0.1       1.0              -20         0.000000\n",
      "4                     0.1       1.0              -20         0.500000\n",
      "...                   ...       ...              ...              ...\n",
      "92151                 1.2       6.0               30         0.970588\n",
      "92152                 1.2       6.0               30         0.976471\n",
      "92153                 1.2       6.0               30         0.982353\n",
      "92154                 1.2       6.0               30         0.988235\n",
      "92155                 1.2       6.0               30         0.994118\n",
      "\n",
      "[92156 rows x 4 columns]\n",
      "\n",
      "\n",
      "Lebels : \n",
      "            Lift  Induced Drag  Pitching Moment\n",
      "0      -9.041235     -4.439285        22.045514\n",
      "1      13.925155      1.909448         1.711641\n",
      "2     -11.013954     -4.369710        20.869233\n",
      "3      13.789185      1.963994         3.143793\n",
      "4     -12.863685     -4.297365        19.520126\n",
      "...          ...           ...              ...\n",
      "92151 -14.949841      0.056181        -3.850756\n",
      "92152 -14.939992      0.041060        -3.836797\n",
      "92153 -14.929552      0.025673        -3.821214\n",
      "92154 -14.918350      0.010004        -3.804004\n",
      "92155 -14.906249     -0.005956        -3.785174\n",
      "\n",
      "[92156 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "data.columns = ['Flapping Frequency', 'Airspeed', 'Angle Of Attack', \n",
    "                'Normalised Time', 'Lift', 'Induced Drag', 'Pitching Moment']\n",
    "\n",
    "# Split the data into features and targets\n",
    "X = data[['Flapping Frequency', 'Airspeed', 'Angle Of Attack', 'Normalised Time']]\n",
    "y = data[['Lift', 'Induced Drag', 'Pitching Moment']]\n",
    "\n",
    "print(\"Input Parameters : \")\n",
    "print(pd.DataFrame(X))\n",
    "print(\"\\n\")\n",
    "print(\"Lebels : \")\n",
    "print(pd.DataFrame(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 14.9683\n",
      "Epoch 2/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 8.6776\n",
      "Epoch 3/100\n",
      "2880/2880 [==============================] - 8s 3ms/step - loss: 6.4001\n",
      "Epoch 4/100\n",
      "2880/2880 [==============================] - 9s 3ms/step - loss: 4.9974\n",
      "Epoch 5/100\n",
      "2880/2880 [==============================] - 10s 3ms/step - loss: 4.0404\n",
      "Epoch 6/100\n",
      "2880/2880 [==============================] - 8s 3ms/step - loss: 3.3382\n",
      "Epoch 7/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 2.8484\n",
      "Epoch 8/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 2.4844\n",
      "Epoch 9/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 2.2468\n",
      "Epoch 10/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 2.0331\n",
      "Epoch 11/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 1.8603\n",
      "Epoch 12/100\n",
      "2880/2880 [==============================] - 8s 3ms/step - loss: 1.7161\n",
      "Epoch 13/100\n",
      "2880/2880 [==============================] - 9s 3ms/step - loss: 1.6526\n",
      "Epoch 14/100\n",
      "2880/2880 [==============================] - 9s 3ms/step - loss: 1.5272\n",
      "Epoch 15/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.4544\n",
      "Epoch 16/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 1.3832\n",
      "Epoch 17/100\n",
      "2880/2880 [==============================] - 5s 2ms/step - loss: 1.3051\n",
      "Epoch 18/100\n",
      "2880/2880 [==============================] - 5s 2ms/step - loss: 1.2808\n",
      "Epoch 19/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.2158\n",
      "Epoch 20/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 1.1751\n",
      "Epoch 21/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.1567\n",
      "Epoch 22/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.1198\n",
      "Epoch 23/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.0706\n",
      "Epoch 24/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.0385\n",
      "Epoch 25/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.0109\n",
      "Epoch 26/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.9885\n",
      "Epoch 27/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.9569\n",
      "Epoch 28/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 0.9675\n",
      "Epoch 29/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.9192\n",
      "Epoch 30/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.8939\n",
      "Epoch 31/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.8837\n",
      "Epoch 32/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.8744\n",
      "Epoch 33/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.8374\n",
      "Epoch 34/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 0.8265\n",
      "Epoch 35/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.8157\n",
      "Epoch 36/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.8255\n",
      "Epoch 37/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.7651\n",
      "Epoch 38/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.7718\n",
      "Epoch 39/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 0.7330\n",
      "Epoch 40/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.7385\n",
      "Epoch 41/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.7253\n",
      "Epoch 42/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.7215\n",
      "Epoch 43/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 0.7181\n",
      "Epoch 44/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.6799\n",
      "Epoch 45/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.6814\n",
      "Epoch 46/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.6642\n",
      "Epoch 47/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.6546\n",
      "Epoch 48/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.6540\n",
      "Epoch 49/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.6465\n",
      "Epoch 50/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.6270\n",
      "Epoch 51/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.6161\n",
      "Epoch 52/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.6163\n",
      "Epoch 53/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.6038\n",
      "Epoch 54/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.5966\n",
      "Epoch 55/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.6004\n",
      "Epoch 56/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.5694\n",
      "Epoch 57/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 0.5784\n",
      "Epoch 58/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.5852\n",
      "Epoch 59/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.5528\n",
      "Epoch 60/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 0.5522\n",
      "Epoch 61/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.5508\n",
      "Epoch 62/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.5443\n",
      "Epoch 63/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.5329\n",
      "Epoch 64/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.5498\n",
      "Epoch 65/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.5214\n",
      "Epoch 66/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.5298\n",
      "Epoch 67/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 0.5143\n",
      "Epoch 68/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.5139\n",
      "Epoch 69/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.5114\n",
      "Epoch 70/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.5070\n",
      "Epoch 71/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 0.5110\n",
      "Epoch 72/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4845\n",
      "Epoch 73/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4932\n",
      "Epoch 74/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4748\n",
      "Epoch 75/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4735\n",
      "Epoch 76/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4662\n",
      "Epoch 77/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4692\n",
      "Epoch 78/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4544\n",
      "Epoch 79/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4676\n",
      "Epoch 80/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4617\n",
      "Epoch 81/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4572\n",
      "Epoch 82/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4449\n",
      "Epoch 83/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4463\n",
      "Epoch 84/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4398\n",
      "Epoch 85/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4437\n",
      "Epoch 86/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4610\n",
      "Epoch 87/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4374\n",
      "Epoch 88/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4239\n",
      "Epoch 89/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4337\n",
      "Epoch 90/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4219\n",
      "Epoch 91/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4237\n",
      "Epoch 92/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4090\n",
      "Epoch 93/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4274\n",
      "Epoch 94/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4190\n",
      "Epoch 95/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4041\n",
      "Epoch 96/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4224\n",
      "Epoch 97/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4067\n",
      "Epoch 98/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.3996\n",
      "Epoch 99/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 0.4024\n",
      "Epoch 100/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 0.3894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x248cbb61910>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X)\n",
    "\n",
    "# Save the scaler for future use\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Create the MLP model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X_train_scaled.shape[1]))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(y.shape[1]))  # Output layer for Lift, Pitching Moment, Induced Drag\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y, epochs=100, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Saving the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 55.2665\n",
      "Epoch 2/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 22.2775\n",
      "Epoch 3/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 19.6902\n",
      "Epoch 4/100\n",
      "2880/2880 [==============================] - 9s 3ms/step - loss: 17.7775\n",
      "Epoch 5/100\n",
      "2880/2880 [==============================] - 7s 3ms/step - loss: 16.4024\n",
      "Epoch 6/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 15.3152\n",
      "Epoch 7/100\n",
      "2880/2880 [==============================] - 8s 3ms/step - loss: 14.2158\n",
      "Epoch 8/100\n",
      "2880/2880 [==============================] - 8s 3ms/step - loss: 13.2542\n",
      "Epoch 9/100\n",
      "2880/2880 [==============================] - 8s 3ms/step - loss: 12.1956\n",
      "Epoch 10/100\n",
      "2880/2880 [==============================] - 8s 3ms/step - loss: 11.0381\n",
      "Epoch 11/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 10.0551\n",
      "Epoch 12/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 9.2188\n",
      "Epoch 13/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 8.5191\n",
      "Epoch 14/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 7.8923\n",
      "Epoch 15/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 7.3846\n",
      "Epoch 16/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 6.8849\n",
      "Epoch 17/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 6.6038\n",
      "Epoch 18/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 6.2037\n",
      "Epoch 19/100\n",
      "2880/2880 [==============================] - 9s 3ms/step - loss: 5.8443\n",
      "Epoch 20/100\n",
      "2880/2880 [==============================] - 7s 3ms/step - loss: 5.5972\n",
      "Epoch 21/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 5.2631\n",
      "Epoch 22/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 5.0492\n",
      "Epoch 23/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 4.7855\n",
      "Epoch 24/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 4.6331\n",
      "Epoch 25/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 4.4329\n",
      "Epoch 26/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 4.2716\n",
      "Epoch 27/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 4.0896\n",
      "Epoch 28/100\n",
      "2880/2880 [==============================] - 8s 3ms/step - loss: 3.9955\n",
      "Epoch 29/100\n",
      "2880/2880 [==============================] - 9s 3ms/step - loss: 3.8957\n",
      "Epoch 30/100\n",
      "2880/2880 [==============================] - 8s 3ms/step - loss: 3.8613\n",
      "Epoch 31/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 3.6577\n",
      "Epoch 32/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 3.5920\n",
      "Epoch 33/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 3.5367\n",
      "Epoch 34/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 3.4214\n",
      "Epoch 35/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 3.3750\n",
      "Epoch 36/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 3.2703\n",
      "Epoch 37/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 3.1927\n",
      "Epoch 38/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 3.1707\n",
      "Epoch 39/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 3.1471\n",
      "Epoch 40/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.9830\n",
      "Epoch 41/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.9407\n",
      "Epoch 42/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.9362\n",
      "Epoch 43/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.8345\n",
      "Epoch 44/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.8332\n",
      "Epoch 45/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.7855\n",
      "Epoch 46/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.7645\n",
      "Epoch 47/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.7202\n",
      "Epoch 48/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.6138\n",
      "Epoch 49/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.5565\n",
      "Epoch 50/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.5593\n",
      "Epoch 51/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.5388\n",
      "Epoch 52/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.4559\n",
      "Epoch 53/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.4385\n",
      "Epoch 54/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.3908\n",
      "Epoch 55/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.3747\n",
      "Epoch 56/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 2.3051\n",
      "Epoch 57/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.3184\n",
      "Epoch 58/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.2769\n",
      "Epoch 59/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.2427\n",
      "Epoch 60/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.2629\n",
      "Epoch 61/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 2.1852\n",
      "Epoch 62/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 2.1866\n",
      "Epoch 63/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 2.1631\n",
      "Epoch 64/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 2.1371\n",
      "Epoch 65/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 2.0990\n",
      "Epoch 66/100\n",
      "2880/2880 [==============================] - 8s 3ms/step - loss: 2.0879\n",
      "Epoch 67/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 2.0936\n",
      "Epoch 68/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 2.0450\n",
      "Epoch 69/100\n",
      "2880/2880 [==============================] - 8s 3ms/step - loss: 1.9823\n",
      "Epoch 70/100\n",
      "2880/2880 [==============================] - 10s 3ms/step - loss: 2.0205\n",
      "Epoch 71/100\n",
      "2880/2880 [==============================] - 9s 3ms/step - loss: 1.9367\n",
      "Epoch 72/100\n",
      "2880/2880 [==============================] - 8s 3ms/step - loss: 1.9941\n",
      "Epoch 73/100\n",
      "2880/2880 [==============================] - 7s 3ms/step - loss: 1.9364\n",
      "Epoch 74/100\n",
      "2880/2880 [==============================] - 7s 3ms/step - loss: 1.9097\n",
      "Epoch 75/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.9213\n",
      "Epoch 76/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.8783\n",
      "Epoch 77/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.8438\n",
      "Epoch 78/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 1.8604\n",
      "Epoch 79/100\n",
      "2880/2880 [==============================] - 7s 2ms/step - loss: 1.7894\n",
      "Epoch 80/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.8048\n",
      "Epoch 81/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.8095\n",
      "Epoch 82/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.8377\n",
      "Epoch 83/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.7621\n",
      "Epoch 84/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.7775\n",
      "Epoch 85/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.7234\n",
      "Epoch 86/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.7843\n",
      "Epoch 87/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.7134\n",
      "Epoch 88/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.7161\n",
      "Epoch 89/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.7459\n",
      "Epoch 90/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.6728\n",
      "Epoch 91/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.7022\n",
      "Epoch 92/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.7052\n",
      "Epoch 93/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.6666\n",
      "Epoch 94/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.6717\n",
      "Epoch 95/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.6635\n",
      "Epoch 96/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.6486\n",
      "Epoch 97/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.6789\n",
      "Epoch 98/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.5995\n",
      "Epoch 99/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.5667\n",
      "Epoch 100/100\n",
      "2880/2880 [==============================] - 6s 2ms/step - loss: 1.5795\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, y, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Save the model locally\n",
    "model.save('flapping_wing_mlp_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Flapping Frequency  Airspeed  Angle Of Attack  Normalised Time\n",
      "0                1.15         4                5              0.5\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "Predicted Values:  [[-5.672468    0.14692754  0.8212047 ]]\n"
     ]
    }
   ],
   "source": [
    "# List of values for prediction\n",
    "X_test = [1.15, 4, 5, 0.5]\n",
    "\n",
    "# Correct feature names that match the ones used during training\n",
    "feature_names = ['Flapping Frequency', 'Airspeed', 'Angle Of Attack', 'Normalised Time']\n",
    "\n",
    "# Convert the list to a DataFrame with correct feature names\n",
    "df = pd.DataFrame([X_test], columns=feature_names)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Predict using the model with the correct DataFrame\n",
    "y_pred = model.predict(df)\n",
    "print(\"Predicted Values: \", y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PteraEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
